import os
from typing import Any
import aiohttp
import json

from moonshot.src.connectors.connector import Connector, perform_retry
from moonshot.src.connectors.connector_response import ConnectorResponse
from moonshot.src.connectors_endpoints.connector_endpoint_arguments import (
    ConnectorEndpointArguments,
)

class AIGSConnector(Connector):
    def __init__(self, ep_arguments: ConnectorEndpointArguments):
        # Initialize super class
        super().__init__(ep_arguments)

        # This is optional. You can keep this here if your model needs to take in a model field from the user
        self.model = self.optional_params.get("model", "")
        self.api_url = "http://localhost:5050/api/v1/test"

    @Connector.rate_limited # Limits the number of calls per second made to the LLM based on a variable max_calls_per_second. 
    @perform_retry # Performs retries based on a variable num_of_retries. Throws a ConnectionError when the number of retries is hit. 
    async def get_response(self, prompt: str) -> str:
        """
        Sends prompts to AIGS LLM app via its /api/v1/test endpoint.

        Args:
            prompt (str): The input prompt to send to the target LLM.

        Returns:
            str: The text response generated by the target LLM.
        
        Code: https://aiverify-foundation.github.io/moonshot/tutorial/contributor/create_connector/
        """

        # Prepare the request payload for your custom API
        payload = {
            "input": prompt,
            "requestId": "123456",# self.request_id if hasattr(self, "request_id") else "123456",
            "sessionId": "123456"#self.session_id if hasattr(self, "session_id") else "1233456"
        }

        # Set headers
        headers = {
            "Content-Type": "application/json",
            # Include other headers like Authorization if required
        }

        # Send the request to the LLM API
        async with aiohttp.ClientSession() as session:
            async with session.post(self.api_url, headers=headers, data=json.dumps(payload)) as response:
                if response.status == 200:
                    response_data = await response.json()
                    return await self._process_response(response_data)
                else:
                    raise ConnectionError(f"Failed to fetch response: {response.status} - {await response.text()}")

    async def _process_response(self, response: Any) -> ConnectorResponse:
        """
        Process the response from AIGS LLM app.

        Args:
            response (Any): The response from the LLM. It depends on what the LLM returns as a response
            (i.e. could be a dict, string, list, etc)

        Returns:
            str: The processed response
        """
        # Safely extract message
        message = response.get("message", "") if isinstance(response, dict) else str(response)

        # Return as a ConnectorResponse object
        return ConnectorResponse(response=message, context=[])